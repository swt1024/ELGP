{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e57edae",
   "metadata": {},
   "source": [
    "## Obtaining essential lncRNA genes in different tissues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0ea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection essential genes for heart saved successfully.\n",
      "Intersection essential genes for lung saved successfully.\n",
      "Intersection essential genes for stomach saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define tissues and model names\n",
    "human_tissues = ['heart','lung','stomach']\n",
    "mouse_tissues = ['heart','lung','brain']\n",
    "model_names = ['SVM', 'MLP']\n",
    "\n",
    "for tissue in human_tissues:\n",
    "    # Load lncRNA data\n",
    "    lnc = pd.read_csv(\"../../data/LPI/human/lncRNA.csv\")\n",
    "    lnc = lnc[['lncRNA_ID', 'gene_id', 'symbol','chr','start','end','strand']]\n",
    "    \n",
    "    # List to store essential genes predicted by each model\n",
    "    all_predictions_ess = []\n",
    "\n",
    "    # Iterate over models\n",
    "    for model in model_names:\n",
    "        # Read prediction file for the current model\n",
    "        prediction = pd.read_csv(f\"../../results/human/{model}_predictions_{tissue}.csv\", dtype='str')\n",
    "        \n",
    "        # Filter for essential genes (Pre_Label == '1')\n",
    "        prediction_ess = prediction[prediction['Pre_Label'] == '1']\n",
    "        prediction_ess = prediction_ess[['lncRNA_ID']]\n",
    "        \n",
    "        # Merge the predicted essential genes with lncRNA data\n",
    "        prediction_ess = pd.merge(prediction_ess, lnc, on='lncRNA_ID', how=\"inner\")\n",
    "        prediction_ess.to_csv(f\"../../results/human/{model}_{tissue}_ess.csv\", index=False)\n",
    "        \n",
    "        # Append the essential genes (as a set) for the current model\n",
    "        all_predictions_ess.append(prediction_ess)  # Use set to store lncRNA_ID for intersection\n",
    "    \n",
    "    # Calculate the intersection (common essential genes across all models)\n",
    "    intersection_ess = all_predictions_ess[0]  # Initialize with the first model's essential genes\n",
    "    for ess_set in all_predictions_ess[1:]:\n",
    "        intersection_ess.merge(ess_set, on='lncRNA_ID', how='inner')\n",
    "    \n",
    "    # Save the result to a CSV file\n",
    "    intersection_ess.to_csv(f\"../../results/human/{tissue}_essential_genes.csv\", index=False)\n",
    "\n",
    "    print(f\"Intersection essential genes for {tissue} saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d173c2c9",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "680a6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define tissues\n",
    "mouse_tissues = ['heart', 'lung', 'brain']\n",
    "human_tissues = ['heart', 'lung', 'stomach']\n",
    "\n",
    "# Initialize an empty DataFrame to store combined results\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Read and concatenate all tissue essential gene files\n",
    "for tissue in mouse_tissues:\n",
    "    df = pd.read_csv(f\"../../results/mouse/{tissue}_essential_genes.csv\")\n",
    "    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Drop duplicate rows based on all columns\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Save the union result to CSV\n",
    "combined_df.to_csv(\"../../results/mouse/mouse_essential_genes_union.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb6d8d",
   "metadata": {},
   "source": [
    "CSV2bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a43adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to convert CSV to valid 6-column BED format\n",
    "def convert_csv_to_bed(csv_file_path, bed_file_path):\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Check required columns\n",
    "    required_columns = ['chr', 'start', 'end', 'lncRNA_ID', 'strand']\n",
    "    if not all(column in df.columns for column in required_columns):\n",
    "        raise ValueError(\"Missing required columns in CSV.\")\n",
    "\n",
    "    # Construct BED columns in correct order\n",
    "    df_bed = pd.DataFrame()\n",
    "    df_bed['chr'] = df['chr']\n",
    "    df_bed['start'] = df['start'].astype(int)\n",
    "    df_bed['end'] = df['end'].astype(int)\n",
    "    df_bed['name'] = df['lncRNA_ID']\n",
    "    df_bed['score'] = 0\n",
    "    df_bed['strand'] = df['strand']\n",
    "\n",
    "    # Save as BED (tab-separated, no header/index)\n",
    "    df_bed.to_csv(bed_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "# Example usage\n",
    "csv_file_path = '../../results/mouse/mouse_essential_genes_union.csv'\n",
    "bed_file_path = 'mouse_essential_genes_union.bed'\n",
    "convert_csv_to_bed(csv_file_path, bed_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f2f0a",
   "metadata": {},
   "source": [
    "### Run get_overlap.sh to find duplicate essential lncRNA genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c36efcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merge completed using longest gene per group. Output saved to: deduplicated_mouse_essential_genes.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Input file paths\n",
    "csv_file = '../../results/mouse/mouse_essential_genes_union.csv'\n",
    "overlap_file = 'mouse_overlapping_genes.txt'\n",
    "output_file = 'deduplicated_mouse_essential_genes.csv'\n",
    "\n",
    "# Load the original annotation table\n",
    "df = pd.read_csv(csv_file)\n",
    "df['length'] = df['end'] - df['start']  # Calculate gene length for selecting representatives\n",
    "\n",
    "# Load the overlapping gene pairs (fully overlapping based on BEDTools results)\n",
    "merge_pairs = pd.read_csv(overlap_file, sep='\\s+', header=None, names=['A', 'B'])\n",
    "\n",
    "# === Build union-find structure (disjoint set) to group overlapping genes ===\n",
    "parent = {}\n",
    "\n",
    "def find(x):\n",
    "    parent.setdefault(x, x)\n",
    "    if parent[x] != x:\n",
    "        parent[x] = find(parent[x])\n",
    "    return parent[x]\n",
    "\n",
    "def union(x, y):\n",
    "    parent[find(y)] = find(x)\n",
    "\n",
    "# Apply union for all overlapping pairs\n",
    "for a, b in zip(merge_pairs['A'], merge_pairs['B']):\n",
    "    union(a, b)\n",
    "\n",
    "# Group all genes by their leader node in the union-find structure\n",
    "groups = defaultdict(set)\n",
    "for gene in set(merge_pairs['A']).union(set(merge_pairs['B'])):\n",
    "    groups[find(gene)].add(gene)\n",
    "\n",
    "# === Determine representative gene per group: longest one ===\n",
    "merge_map = {}  # representative lncRNA_ID → list of merged lncRNA_IDs\n",
    "for group in groups.values():\n",
    "    group_df = df[df['lncRNA_ID'].isin(group)]\n",
    "    rep_row = group_df.loc[group_df['length'].idxmax()]  # select longest gene\n",
    "    rep_id = rep_row['lncRNA_ID']\n",
    "    other_ids = set(group) - {rep_id}\n",
    "    merge_map[rep_id] = list(other_ids)\n",
    "\n",
    "# === Build final output ===\n",
    "# Retain entries that were never merged + representative entries\n",
    "merged_ids = set(merge_pairs['B'])  # IDs that were merged into others\n",
    "all_rep_ids = set(merge_map.keys())\n",
    "retained_ids = set(df['lncRNA_ID']) - merged_ids\n",
    "final_ids = retained_ids.union(all_rep_ids)\n",
    "\n",
    "# Filter the dataframe\n",
    "df_merged = df[df['lncRNA_ID'].isin(final_ids)].copy()\n",
    "\n",
    "# Add a column showing which IDs were merged into each representative\n",
    "df_merged['Merged_IDs'] = df_merged['lncRNA_ID'].apply(lambda x: ';'.join(merge_map[x]) if x in merge_map else '')\n",
    "\n",
    "# Drop the temporary length column\n",
    "df_merged.drop(columns='length', inplace=True)\n",
    "\n",
    "# Save the result\n",
    "df_merged.to_csv(output_file, index=False)\n",
    "print(f\"✅ Merge completed using longest gene per group. Output saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010378d1",
   "metadata": {},
   "source": [
    "## Obtaining essential lncRNA genes in different tissues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcf79418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Step 1: Load the merged file and reconstruct merge_map ===\n",
    "# The file should contain columns: lncRNA_ID, Merged_IDs\n",
    "merged_df = pd.read_csv(\"deduplicated_mouse_essential_genes.csv\")\n",
    "\n",
    "# Build merge_map: representative → [merged_IDs]\n",
    "merge_map = {}\n",
    "\n",
    "for _, row in merged_df.iterrows():\n",
    "    rep_id = row['lncRNA_ID']\n",
    "    if pd.notna(row.get('Merged_IDs')) and row['Merged_IDs'].strip():\n",
    "        merged_list = row['Merged_IDs'].split(';')\n",
    "        merge_map[rep_id] = merged_list\n",
    "\n",
    "# Create reverse map: lncRNA_ID (any member) → representative\n",
    "reverse_map = {}\n",
    "for rep, others in merge_map.items():\n",
    "    reverse_map[rep] = rep  # rep maps to itself\n",
    "    for gene in others:\n",
    "        reverse_map[gene] = rep\n",
    "\n",
    "# === Step 2: Load a specific tissue's gene list ===\n",
    "# Replace this with your actual tissue file path\n",
    "tissue_df = pd.read_csv(\"../../results/mouse/brain_essential_genes.csv\")\n",
    "lnc_ids = tissue_df['lncRNA_ID']\n",
    "\n",
    "# === Step 3: Replace lncRNA_IDs with their representative IDs ===\n",
    "representative_ids = lnc_ids.apply(lambda x: reverse_map.get(x, x))\n",
    "\n",
    "# === Step 4: Remove duplicates and save to file ===\n",
    "unique_reps = representative_ids.drop_duplicates().to_frame(name='lncRNA_ID')\n",
    "unique_reps.to_csv(\"deduplicated_mouse_brain_essential_genes.csv\", index=False, header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46252c",
   "metadata": {},
   "source": [
    "------转录本级别的映射，已舍弃------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a192d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing lncRNA_LncBookv2.0_GRCh38.gtf: 1412552 lines [00:10, 135137.41 lines/s]\n",
      "Parsing NONCODEv6_human_hg38_lncRNA.gtf: 608746 lines [00:04, 128774.67 lines/s]\n",
      "Parsing NONCODEv5_human_hg38_lncRNA.gtf: 601456 lines [00:04, 135874.02 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.113.gtf: 4114455 lines [00:25, 161683.06 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.112.gtf: 3464559 lines [00:17, 201826.56 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.111.gtf: 3424902 lines [00:18, 189566.35 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.110.gtf: 3421627 lines [00:16, 206360.74 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.109.gtf: 3420366 lines [00:19, 179730.78 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.108.gtf: 3409311 lines [00:16, 208457.69 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.107.gtf: 3371249 lines [00:17, 192782.84 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.106.gtf: 3279410 lines [00:10, 301738.75 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.104.gtf: 3146137 lines [00:08, 366044.95 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.97.gtf: 2877402 lines [00:09, 306520.23 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.93.gtf: 2689571 lines [00:07, 377409.52 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.87.gtf: 2575499 lines [00:07, 329966.78 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.84.gtf: 2569155 lines [00:08, 315505.77 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.80.gtf: 2748742 lines [00:07, 354027.27 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.78.gtf: 2672006 lines [00:08, 319128.15 lines/s]\n",
      "Parsing Homo_sapiens.GRCh38.76.gtf: 2661884 lines [00:06, 442200.24 lines/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Check 'missing_lncRNAs_human_heart.csv' for lncRNAs without transcripts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration paths\n",
    "ENSEMBL_GTF_DIR = \"../../data/reference_lncRNA/human/gtf/ensembl/\"\n",
    "LNCBOOK_GTF = \"../../data/reference_lncRNA/human/gtf/lncRNA_LncBookv2.0_GRCh38.gtf\"\n",
    "NONCODE_V5_GTF = \"../../data/reference_lncRNA/human/gtf/NONCODEv5_human_hg38_lncRNA.gtf\"\n",
    "NONCODE_V6_GTF = \"../../data/reference_lncRNA/human/gtf/NONCODEv6_human_hg38_lncRNA.gtf\"\n",
    "\n",
    "# Load gene ID mapping\n",
    "lnc_df = pd.read_csv('../../results/human/heart_essential_genes.csv')\n",
    "gene_id_to_lnc = {row['gene_id']: row['lncRNA_ID'] for index, row in lnc_df.iterrows()}\n",
    "symbol_to_lnc = {row['symbol']: row['lncRNA_ID'] for index, row in lnc_df.iterrows()}\n",
    "\n",
    "def process_gtf(input_path, fout, gene_id_to_lnc, symbol_to_lnc, use_symbol=False):\n",
    "    with open(input_path) as f:\n",
    "        for line in tqdm(f, desc=f\"Parsing {os.path.basename(input_path)}\", unit=\" lines\"):\n",
    "            if line.startswith('#') or not line.strip():\n",
    "                continue\n",
    "            segments = line.strip().split('\\t')\n",
    "            if len(segments) < 9 or segments[2] not in {'transcript', 'exon'}:\n",
    "                continue\n",
    "            attributes = segments[8]\n",
    "            gene_id_match = re.search(r'gene_id \"([^\"]+)\"', attributes)\n",
    "            gene_name_match = re.search(r'gene_name \"([^\"]+)\"', attributes)\n",
    "            lncRNA_id = None\n",
    "            if gene_id_match:\n",
    "                gene_id = gene_id_match.group(1).split('.')[0]  # Remove version number if present\n",
    "                if gene_id in gene_id_to_lnc:\n",
    "                    lncRNA_id = gene_id_to_lnc[gene_id]\n",
    "            elif use_symbol and gene_name_match and gene_name_match.group(1) in symbol_to_lnc:\n",
    "                lncRNA_id = symbol_to_lnc[gene_name_match.group(1)]\n",
    "            if lncRNA_id:\n",
    "                all_found_lncRNA_IDs.add(lncRNA_id)\n",
    "                segments[8] = re.sub(r'gene_id \"[^\"]+\"', f'gene_id \"{lncRNA_id}\"', segments[8])\n",
    "                fout.write('\\t'.join(segments) + '\\n')\n",
    "    return {k: v for k, v in gene_id_to_lnc.items() if v not in all_found_lncRNA_IDs}, \\\n",
    "           {k: v for k, v in symbol_to_lnc.items() if v not in all_found_lncRNA_IDs}\n",
    "\n",
    "all_found_lncRNA_IDs = set()\n",
    "with open(\"human_heart.gtf\", 'w') as fout:\n",
    "    # Process specific GTF files\n",
    "    for gtf_path in [LNCBOOK_GTF, NONCODE_V6_GTF, NONCODE_V5_GTF]:\n",
    "        gene_id_to_lnc, symbol_to_lnc = process_gtf(gtf_path, fout, gene_id_to_lnc, symbol_to_lnc, use_symbol=False)\n",
    "    # Process Ensembl files in version order\n",
    "    ensembl_files = sorted(\n",
    "        [f for f in os.listdir(ENSEMBL_GTF_DIR) if f.endswith('.gtf')],\n",
    "        key=lambda x: int(re.search(r'GRCh38\\.(\\d+)\\.gtf', x).group(1)),\n",
    "        reverse=True\n",
    "    )\n",
    "    for ef in ensembl_files:\n",
    "        gene_id_to_lnc, symbol_to_lnc = process_gtf(os.path.join(ENSEMBL_GTF_DIR, ef), fout, gene_id_to_lnc, symbol_to_lnc, use_symbol=True)\n",
    "\n",
    "# Save missing genes\n",
    "missing_lnc_ids = set(lnc_df['lncRNA_ID']) - all_found_lncRNA_IDs\n",
    "missing_df = lnc_df[lnc_df['lncRNA_ID'].isin(missing_lnc_ids)]\n",
    "missing_df.to_csv('missing_lncRNAs_human_heart.csv', index=False)\n",
    "\n",
    "print(\"Processing complete. Check 'missing_lncRNAs_human_heart.csv' for lncRNAs without transcripts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bcc35e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gffcompare completed successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_gffcompare(annotated_gtf, reference_gtf, output_prefix):\n",
    "    \"\"\"\n",
    "    Run gffcompare to compare transcript annotations against a reference.\n",
    "\n",
    "    Parameters:\n",
    "    annotated_gtf (str): Path to the GTF file with your annotations.\n",
    "    reference_gtf (str): Path to the reference GTF file.\n",
    "    output_prefix (str): Prefix for output files generated by gffcompare.\n",
    "    \"\"\"\n",
    "    # Construct the gffcompare command\n",
    "    command = [\n",
    "        \"gffcompare\",\n",
    "        \"-r\", reference_gtf,  # Reference GTF file\n",
    "        \"-o\", output_prefix,  # Output prefix\n",
    "        annotated_gtf         # Annotated GTF file\n",
    "    ]\n",
    "    \n",
    "    # Execute the command\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    \n",
    "    # Check if gffcompare ran successfully\n",
    "    if result.returncode == 0:\n",
    "        print(\"gffcompare completed successfully.\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"Error in gffcompare:\")\n",
    "        print(result.stderr)\n",
    "\n",
    "# Example usage\n",
    "annotated_gtf = \"human_heart.gtf\"\n",
    "reference_gtf = \"gencode.v47.long_noncoding_RNAs.gtf\"\n",
    "output_prefix = \"gffc\"\n",
    "\n",
    "run_gffcompare(annotated_gtf, reference_gtf, output_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3cf19b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strict matches saved to 'human_heart_strict_matches.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def strict_matching(tmap_path):\n",
    "    # 读取 tmap 文件（带 header）\n",
    "    df = pd.read_csv(tmap_path, sep='\\t')\n",
    "\n",
    "    # 定义我们关心的 class_code\n",
    "    match_classes = {'=', 'j', 'k', 'e'}\n",
    "\n",
    "    strict_matches = {}\n",
    "\n",
    "    # 按 qry_gene_id 分组\n",
    "    for qry_gene, group in df.groupby('qry_gene_id'):\n",
    "        class_codes = set(group['class_code'].dropna())\n",
    "\n",
    "        # 要求该基因的所有转录本 class_code 都在允许范围内\n",
    "        if class_codes.issubset(match_classes):\n",
    "            ref_genes = group['ref_gene_id'].dropna().unique()\n",
    "            if len(ref_genes) == 1:\n",
    "                strict_matches[qry_gene] = ref_genes[0]\n",
    "\n",
    "    return strict_matches\n",
    "\n",
    "# 使用\n",
    "tmap_path = 'gffc.human_heart.gtf.tmap'\n",
    "strict_matches = strict_matching(tmap_path)\n",
    "\n",
    "# 保存结果\n",
    "results_df = pd.DataFrame(list(strict_matches.items()), columns=['Query Gene', 'Reference Gene'])\n",
    "results_df.to_csv('human_heart_strict_matches.csv', index=False)\n",
    "print(\"Strict matches saved to 'human_heart_strict_matches.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esslnc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
