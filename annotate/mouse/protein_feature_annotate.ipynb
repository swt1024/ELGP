{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate features for proteins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 Extract all protein. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.1 Extract all protein molecules from a filtered LPPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example usage\n",
    "inter_file = 'inter_with_valid_lnc.csv'\n",
    "inter = pd.read_csv(inter_file)\n",
    "\n",
    "# Concatenate two columns into a new Series and remove duplicates\n",
    "molecule = pd.concat([inter['Node_i'], inter['Node_j']]).reset_index(drop=True)\n",
    "molecule_df = pd.DataFrame(molecule, columns=['molecule'])\n",
    "molecule_df = molecule_df.drop_duplicates()\n",
    "\n",
    "protein_file = '../../data/LPPI/mouse/protein_updated.csv'\n",
    "proteins = pd.read_csv(protein_file)\n",
    "\n",
    "proteins = proteins[proteins['protein_ID'].isin(molecule_df['molecule'])]\n",
    "\n",
    "# Export to CSV file\n",
    "protein_file = 'proteins.csv'\n",
    "proteins.to_csv(protein_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 Annotate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.1 Annotate the number of go terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_gene_go_terms(gaf_file):\n",
    "    # Read GAF (Gene Association Format) file\n",
    "    columns = [\n",
    "        \"DB\", \"DB_Object_ID\", \"DB_Object_Symbol\", \"Qualifier\", \"GO_ID\", \"DB_Reference\", \n",
    "        \"Evidence_Code\", \"With_or_From\", \"Aspect\", \"DB_Object_Name\", \"DB_Object_Synonym\",\n",
    "        \"DB_Object_Type\", \"Taxon\", \"Date\", \"Assigned_By\", \"Annotation_Extension\", \"Gene_Product_Form_ID\"\n",
    "    ]\n",
    "    \n",
    "    # Only read the first 15 columns to avoid issues with different GAF versions\n",
    "    df = pd.read_csv(gaf_file, sep=\"\\t\", comment=\"!\", header=None, names=columns, usecols=range(15), dtype=str)\n",
    "    \n",
    "    # Filter for mouse-specific data (taxon:10090)\n",
    "    df = df[df[\"Taxon\"] == \"taxon:10090\"]\n",
    "\n",
    "    # Keep only gene symbols and GO terms, remove duplicates\n",
    "    df_filtered = df[[\"DB_Object_Symbol\", \"GO_ID\"]].drop_duplicates()\n",
    "    \n",
    "    # Count the number of unique GO terms associated with each gene\n",
    "    gene_go_counts = df_filtered.groupby([\"DB_Object_Symbol\"])[\"GO_ID\"].nunique().reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    gene_go_counts.columns = [\"Gene_Symbol\", \"GO_Term_Count\"]\n",
    "    \n",
    "    return gene_go_counts\n",
    "\n",
    "gaf_file = \"../../omics/protein/mouse/mgi.gaf\"  \n",
    "gene_go_counts = count_gene_go_terms(gaf_file)\n",
    "\n",
    "protein = pd.read_csv(\"proteins.csv\", dtype=str)\n",
    "\n",
    "filtered_gene_go_term_counts = pd.merge(protein, gene_go_counts, left_on='protein', right_on='Gene_Symbol', how='inner')\n",
    "filtered_gene_go_term_counts = filtered_gene_go_term_counts[['protein_ID','GO_Term_Count']]\n",
    "\n",
    "# Save the result to a CSV file\n",
    "filtered_gene_go_term_counts.to_csv(\"protein_go_term_counts.csv\", sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.2 Annotate the number of orthologs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2.2.1 Extract gene name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"proteins.csv\")\n",
    "\n",
    "gene = data[['protein']]\n",
    "\n",
    "gene.to_csv(\"protein_name.txt\", sep=\"\\t\",index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2.2.2 Get ortholog count using Ensembl REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configuration parameters\n",
    "INPUT_FILE = \"protein_name.txt\"\n",
    "OUTPUT_FILE = \"all_species_ortholog_counts.csv\"\n",
    "THREADS = 5\n",
    "MAX_RETRIES = 3\n",
    "SLEEP_BETWEEN = 0.4\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "def count_orthologs_by_symbol(symbol):\n",
    "    url = f\"https://rest.ensembl.org/homology/symbol/mus_musculus/{symbol}?type=orthologues\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=15)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                homologies = data['data'][0].get('homologies', [])\n",
    "                count = sum(1 for h in homologies if h['type'] == 'ortholog_one2one')\n",
    "                return (symbol, count, \"OK\")\n",
    "            elif r.status_code == 404:\n",
    "                return (symbol, 0, \"NotFound\")\n",
    "            else:\n",
    "                print(f\"[{symbol}] HTTP {r.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{symbol}] error: {e}\")\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "    return (symbol, 0, \"Error\")\n",
    "\n",
    "def load_symbols():\n",
    "    with open(INPUT_FILE) as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def load_existing_results():\n",
    "    if not os.path.exists(OUTPUT_FILE):\n",
    "        return set()\n",
    "    df = pd.read_csv(OUTPUT_FILE)\n",
    "    return set(df['GeneSymbol'].values)\n",
    "\n",
    "def save_result(symbol, count, status):\n",
    "    with open(OUTPUT_FILE, \"a\") as f:\n",
    "        f.write(f\"{symbol},{count},{status}\\n\")\n",
    "\n",
    "def main():\n",
    "    all_symbols = load_symbols()\n",
    "    done_symbols = load_existing_results()\n",
    "    symbols_to_query = [s for s in all_symbols if s not in done_symbols]\n",
    "\n",
    "    print(f\"\\n Total genes: {len(all_symbols)}\")\n",
    "    print(f\" Already processed: {len(done_symbols)}\")\n",
    "    print(f\" Pending: {len(symbols_to_query)}\")\n",
    "    print(f\" Starting concurrent queries with {THREADS} threads\\n\")\n",
    "\n",
    "    with open(OUTPUT_FILE, \"a\") as f:\n",
    "        if os.stat(OUTPUT_FILE).st_size == 0:\n",
    "            f.write(\"GeneSymbol,OrthologCount,Status\\n\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
    "        future_to_symbol = {\n",
    "            executor.submit(count_orthologs_by_symbol, symbol): symbol\n",
    "            for symbol in symbols_to_query\n",
    "        }\n",
    "\n",
    "        for future in as_completed(future_to_symbol):\n",
    "            symbol = future_to_symbol[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                save_result(*result)\n",
    "                print(f\" {result[0]} → {result[1]} orthologs\")\n",
    "            except Exception as exc:\n",
    "                print(f\" {symbol} exception: {exc}\")\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "    print(f\"\\n Query complete! Results saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2.2.3 Annotate ortholog count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "orthologs_counts = pd.read_csv('all_species_ortholog_counts.csv')\n",
    "protein = pd.read_csv('proteins.csv')\n",
    "\n",
    "orthologs_counts = orthologs_counts[orthologs_counts['Status']=='OK']\n",
    "orthologs_counts = orthologs_counts[['GeneSymbol','OrthologCount']]\n",
    "protein_orthologs_counts = pd.merge(protein,orthologs_counts,left_on='protein',right_on='GeneSymbol',how='inner')\n",
    "protein_orthologs_counts = protein_orthologs_counts[['protein_ID','OrthologCount']]\n",
    "protein_orthologs_counts.to_csv(\"protein_orthologs_counts.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.3 Annotate the expression feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2.3.1 Calculate avg_TPM of gene at 22 tissues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_120245/3889468904.py:3: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exp_file = pd.read_csv(\"../../omics/protein/mouse/filtered_exp.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "exp_file = pd.read_csv(\"../../omics/protein/mouse/filtered_exp.csv\")\n",
    "\n",
    "# Ensure column names are correct (remove leading/trailing spaces or capitalization issues)\n",
    "exp_file.columns = exp_file.columns.str.strip()\n",
    "\n",
    "# Calculate the mean of `avg_TPM` grouped by `Gene Symbol` and `Anatomical Structure`\n",
    "grouped_avg = exp_file.groupby(['Gene Symbol', 'Anatomical Structure'])['avg_TPM'].mean().reset_index()\n",
    "grouped_avg.rename(columns={'avg_TPM': 'Mean_avg_TPM'}, inplace=True)\n",
    "\n",
    "# Compute the median and mean of the above means, grouped by `Gene Symbol`\n",
    "final_stats = grouped_avg.groupby('Gene Symbol')['Mean_avg_TPM'].agg(['median', 'mean']).reset_index()\n",
    "final_stats.rename(columns={'median': 'Median_avg_TPM', 'mean': 'Mean_avg_TPM'}, inplace=True)\n",
    "\n",
    "final_stats.to_csv(\"exp.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step2.3.2 Annotate the mean&media of expression at 22 tissues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "exp_file = pd.read_csv(\"exp.csv\")\n",
    "protein = pd.read_csv(\"proteins.csv\", dtype=str)\n",
    "protein_exp = pd.merge(protein, exp_file, left_on='protein', right_on='Gene Symbol', how='inner')\n",
    "\n",
    "protein_exp = protein_exp[['protein_ID','Median_avg_TPM','Mean_avg_TPM']]\n",
    "protein_exp.to_csv(\"protein_exp.csv\", sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 Merge annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "exp_anno = pd.read_csv(\"protein_exp.csv\")\n",
    "go_anno = pd.read_csv(\"protein_go_term_counts.csv\")\n",
    "homo_anno = pd.read_csv(\"protein_orthologs_counts.csv\")\n",
    "\n",
    "protein_annotation = pd.merge(exp_anno, go_anno, on='protein_ID', how=\"inner\")\n",
    "protein_annotation = protein_annotation.merge(homo_anno, on=\"protein_ID\", how='inner')\n",
    "\n",
    "ID_column = protein_annotation.iloc[:, [0]]\n",
    "feature_columns = protein_annotation.iloc[:, 1:]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(feature_columns)\n",
    "\n",
    "df_normalized = pd.concat([ID_column, pd.DataFrame(normalized_data, columns=feature_columns.columns)], axis=1)\n",
    "\n",
    "df_normalized.to_csv(\"transformed_protein_annotation.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3.2 Get inter with valid protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inter = pd.read_csv(\"inter_with_valid_lnc.csv\")\n",
    "valid_protein = pd.read_csv(\"transformed_protein_annotation.csv\")\n",
    "\n",
    "valid_protein_set = set(valid_protein['protein_ID'])\n",
    "\n",
    "# Define a function to check whether nodes starting with 'p' exist in valid_protein\n",
    "def check_valid_protein(node):\n",
    "    if node.startswith('p'):\n",
    "        return node in valid_protein_set\n",
    "    return True  # Keep nodes that do not start with 'p'\n",
    "\n",
    "# Check each row: both Node_i and Node_j must be valid\n",
    "inter = inter[inter['Node_i'].apply(check_valid_protein) & inter['Node_j'].apply(check_valid_protein)]\n",
    "\n",
    "inter.to_csv(\"valid_inter.csv\", index=False)\n",
    "\n",
    "tissues = ['heart', 'lung', 'brain']\n",
    "for tissue in tissues:\n",
    "    lnc = pd.read_csv(f\"{tissue}_annotation.csv\")\n",
    "    lnc = lnc[lnc['lncRNA_ID'].isin(inter['Node_i'])]\n",
    "\n",
    "    lnc.to_csv(f\"valid_{tissue}_annotation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 Calculate edge wight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted interaction file has been successfully saved!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the input CSV file\n",
    "inter = pd.read_csv(\"valid_inter.csv\")\n",
    "\n",
    "# Ensure (Node_i, Node_j) and (Node_j, Node_i) are considered the same edge\n",
    "# Sort each pair so that the smaller node always comes first\n",
    "inter[['Node_i', 'Node_j']] = inter[['Node_i', 'Node_j']].apply(lambda x: tuple(sorted(x)), axis=1, result_type='expand')\n",
    "\n",
    "# Compute the weight (number of occurrences of each edge)\n",
    "inter['weight'] = inter.groupby(['Node_i', 'Node_j']).transform('size')\n",
    "\n",
    "# Remove duplicate edges, keeping only one occurrence per (Node_i, Node_j) pair\n",
    "inter = inter.drop_duplicates(subset=['Node_i', 'Node_j'])\n",
    "\n",
    "inter.columns = ['source', 'target', 'weight']\n",
    "\n",
    "# Save the weighted edge list to a new CSV file\n",
    "inter.to_csv('weighted_valid_inter.csv', index=False)\n",
    "\n",
    "print(\"Weighted interaction file has been successfully saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "边的统计：\n",
      "Edge Type\n",
      "LPI    139935\n",
      "PPI     76235\n",
      "Name: count, dtype: int64\n",
      "\n",
      "每种边中每个节点的独特数量：\n",
      "LPI:\n",
      "  lncRNA: 37853\n",
      "  Protein: 180\n",
      "PPI:\n",
      "  Protein: 11498\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载CSV文件\n",
    "df = pd.read_csv('valid_inter.csv')\n",
    "\n",
    "# 创建节点类型列\n",
    "df['Type1'] = df['Node_i'].apply(lambda x: 'lncRNA' if x.startswith('l') else 'Protein')\n",
    "df['Type2'] = df['Node_j'].apply(lambda x: 'lncRNA' if x.startswith('l') else 'Protein')\n",
    "\n",
    "# 创建边的类型标签，这里设置axis=1以按行应用函数\n",
    "df['Edge Type'] = df.apply(lambda row: 'LPI' if (row['Type1'] != row['Type2']) else 'PPI', axis=1)\n",
    "\n",
    "# 初始化统计每种边中每种节点的独特数量\n",
    "edge_node_unique_counts = {\n",
    "    'LPI': {'lncRNA': set(), 'Protein': set()},\n",
    "    'PPI': {'Protein': set()}\n",
    "}\n",
    "\n",
    "# 对每种边进行遍历，收集独特节点\n",
    "for index, row in df.iterrows():\n",
    "    edge_type = row['Edge Type']\n",
    "    edge_node_unique_counts[edge_type][row['Type1']].add(row['Node_i'])\n",
    "    edge_node_unique_counts[edge_type][row['Type2']].add(row['Node_j'])\n",
    "\n",
    "# 计算每种边的数量\n",
    "edge_counts = df['Edge Type'].value_counts()\n",
    "\n",
    "# 输出每种边的统计结果\n",
    "print(\"边的统计：\")\n",
    "print(edge_counts)\n",
    "print(\"\\n每种边中每个节点的独特数量：\")\n",
    "for edge_type, nodes in edge_node_unique_counts.items():\n",
    "    print(f\"{edge_type}:\")\n",
    "    for node_type, node_set in nodes.items():\n",
    "        print(f\"  {node_type}: {len(node_set)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esslnc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
