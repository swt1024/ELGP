{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotate omics features for lncRNAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 Get genomic position for lncRNAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1.1 Convert 0-based coordination to 1-based coordination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inter_lnc = pd.read_csv('../../data/LPI/mouse/lncRNA.csv')\n",
    "inter_lnc = inter_lnc[['lncRNA_ID', 'chr', 'start', 'end', 'strand']]\n",
    "\n",
    "inter_lnc.to_csv('lnc_with_position_0-based.csv', index=False) \n",
    "\n",
    "# convert 0-based position to 1-based position\n",
    "inter_lnc['start'] = inter_lnc['start'] + 1\n",
    "\n",
    "inter_lnc.to_csv('lnc_with_position_1-based.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2 Annotate histone marks, DHS and CTCF binding site for lncRNAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2.1 Calculate features of epigenomic marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing brain - DHS ...\n",
      "Processing brain - H3K4me3 ...\n",
      "Processing brain - H3K9me3 ...\n",
      "Processing brain - H3K27ac ...\n",
      "Processing brain - H3K36me3 ...\n",
      "Processing brain - H3K27me3 ...\n"
     ]
    }
   ],
   "source": [
    "import pyBigWig\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File paths\n",
    "lncRNA_file = \"lnc_with_position_0-based.csv\"\n",
    "epi_folder = \"../../omics/ENCODE_annotation/mouse/\"\n",
    "\n",
    "\n",
    "# Read lncRNA coordinate information\n",
    "lncRNAs = pd.read_csv(lncRNA_file)\n",
    "\n",
    "# Store lncRNA IDs that failed to extract features\n",
    "failed_lncRNA_ids = set()\n",
    "\n",
    "# List of tissue and epigenetic features\n",
    "tissues = ['heart', 'lung', 'brain']\n",
    "epi_features = ['DHS', 'H3K4me3', 'H3K9me3', 'H3K27ac', 'H3K36me3', 'H3K27me3']\n",
    "\n",
    "# Function to calculate both peak counts and max signal value\n",
    "def calculate_features(row, bb):\n",
    "    chrom = str(row[\"chr\"])\n",
    "    start = int(row[\"start\"])\n",
    "    end = int(row[\"end\"])\n",
    "    \n",
    "    try:\n",
    "        entries = bb.entries(chrom, start, end)  # Extract peak intervals\n",
    "        if entries is None:\n",
    "            return 0, 0  # No peaks, return (0, 0)\n",
    "        \n",
    "        peak_count = len(entries)  # Count peaks\n",
    "        signal_values = []\n",
    "        \n",
    "        # Extract signalValues (assuming details is space/tab-separated and signalValue is at index 6)\n",
    "        for entry in entries:\n",
    "            details = entry[2]  # Get the details string\n",
    "            details_parts = details.split()  # Split by space or tab\n",
    "            if len(details_parts) > 6:  # Ensure there are enough fields\n",
    "                signal_value = float(details_parts[6])  # NarrowPeak format: signalValue is usually at index 6\n",
    "                signal_values.append(signal_value)\n",
    "\n",
    "        max_signal_value = max(signal_values) if signal_values else 0  # Get max signalValue\n",
    "        return peak_count, max_signal_value\n",
    "    \n",
    "    except Exception as e:\n",
    "        return 0, 0  # Return (0,0) in case of failure\n",
    "\n",
    "# Iterate through all tissue folders\n",
    "for tissue in tissues:\n",
    "    # Initialize DataFrame to store all features\n",
    "    combined_features = lncRNAs.copy()\n",
    "    cell_folder = os.path.join(epi_folder, tissue)\n",
    "    if not os.path.isdir(cell_folder):\n",
    "        print(f\"Warning: Cell line folder {cell_folder} does not exist, skipping.\")\n",
    "    else:\n",
    "        # Iterate through all epigenetic feature bigBed files\n",
    "        for epi_feature in epi_features:\n",
    "            bigbed_file = os.path.join(cell_folder, f\"{epi_feature}.bigbed\")\n",
    "            if not os.path.exists(bigbed_file):\n",
    "                print(f\"Warning: File {bigbed_file} does not exist, skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Open the bigBed file\n",
    "            try:\n",
    "                bb = pyBigWig.open(bigbed_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to open file {bigbed_file}, error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing {tissue} - {epi_feature} ...\")\n",
    "            \n",
    "            # Calculate both features per lncRNA region\n",
    "            results = lncRNAs.apply(calculate_features, axis=1, bb=bb)\n",
    "\n",
    "            # Store results in two separate columns\n",
    "            combined_features[f\"{tissue}_{epi_feature}_peak_counts\"] = results.apply(lambda x: x[0])  # Peak counts\n",
    "            combined_features[f\"{tissue}_{epi_feature}_max_signalValue\"] = results.apply(lambda x: x[1])  # Max signal value\n",
    "            \n",
    "            # Close the bigBed file\n",
    "            bb.close()\n",
    "\n",
    "    output_file = f\"{tissue}_epi.csv\"\n",
    "\n",
    "    # Remove unnecessary columns and save results\n",
    "    combined_features = combined_features.drop(columns=['chr', 'start', 'end', 'strand'])\n",
    "    combined_features.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3 Get conversation score for lncRNAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3.1 Calculate feature of conservation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phyloP.bw ...\n",
      "Processing phastCons.bw ...\n",
      "Conservation scores calculated and saved.\n",
      "conservation_scores.csv contains 37854 records.\n",
      "no_conservation_scores.csv contains 279 missing lncRNA IDs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_scores(df, bw_path, score_name):\n",
    "    \"\"\"\n",
    "    Extracts conservation scores from a bigWig file.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing lncRNA information.\n",
    "        bw_path (str): Path to the bigWig file.\n",
    "        score_name (str): Name of the conservation score.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with mean and max scores.\n",
    "        set: Set of lncRNA_IDs without scores.\n",
    "    \"\"\"\n",
    "    scores_df = df.copy()\n",
    "    scores_df[f'{score_name}_mean_score'] = np.nan\n",
    "    scores_df[f'{score_name}_max_score'] = np.nan\n",
    "\n",
    "    no_score_ids = set()\n",
    "\n",
    "    try:\n",
    "        with pyBigWig.open(bw_path) as bw:\n",
    "            for index, row in df.iterrows():\n",
    "                chrom = row['chr']\n",
    "                if chrom in bw.chroms():\n",
    "                    scores = bw.values(chrom, int(row['start']), int(row['end']), numpy=True)\n",
    "                    if scores.size > 0 and not np.all(np.isnan(scores)):\n",
    "                        scores_df.at[index, f'{score_name}_mean_score'] = np.nanmean(scores)\n",
    "                        scores_df.at[index, f'{score_name}_max_score'] = np.nanmax(scores)\n",
    "                    else:\n",
    "                        no_score_ids.add(row['lncRNA_ID'])\n",
    "                else:\n",
    "                    no_score_ids.add(row['lncRNA_ID'])\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to open {bw_path}: {e}\")\n",
    "\n",
    "    return scores_df, no_score_ids\n",
    "\n",
    "def calculate_conservation_features(lncRNAs_csv, conservation_dir):\n",
    "    \"\"\"\n",
    "    Calculates conservation features from all bigWig files in a given directory.\n",
    "\n",
    "    Args:\n",
    "        lncRNAs_csv (str): Path to lncRNA CSV file.\n",
    "        conservation_dir (str): Directory containing bigWig files.\n",
    "\n",
    "    Outputs:\n",
    "        - lnc_with_conservation_scores.csv: lncRNAs with conservation scores.\n",
    "        - lnc_without_conservation_scores.csv: lncRNAs without available scores.\n",
    "    \"\"\"\n",
    "    lncRNAs = pd.read_csv(lncRNAs_csv)\n",
    "    missing_scores_ids = set()\n",
    "\n",
    "    # List all bigWig files in the directory\n",
    "    bw_files = [f for f in os.listdir(conservation_dir) if f.endswith(('.bw', '.bigWig'))]\n",
    "\n",
    "    if not bw_files:\n",
    "        print(\" No bigWig files found in the directory.\")\n",
    "        return\n",
    "\n",
    "    # Initialize lnc_with_score with lncRNA IDs\n",
    "    lnc_with_score = lncRNAs.copy()\n",
    "\n",
    "    # Process each bigWig file\n",
    "    for bw_file in bw_files:\n",
    "        bw_path = os.path.join(conservation_dir, bw_file)\n",
    "        score_name = os.path.splitext(bw_file)[0]  # Use filename as score name\n",
    "\n",
    "        print(f\"Processing {bw_file} ...\")\n",
    "\n",
    "        scores_df, no_score_ids = get_scores(lnc_with_score, bw_path, score_name)\n",
    "        missing_scores_ids.update(no_score_ids)\n",
    "\n",
    "        # Merge the new scores into the main DataFrame\n",
    "        lnc_with_score = pd.merge(lnc_with_score, scores_df[[ 'lncRNA_ID', \n",
    "                                                              f'{score_name}_mean_score', \n",
    "                                                              f'{score_name}_max_score']], \n",
    "                                  on='lncRNA_ID', how='left')\n",
    "\n",
    "    # Remove lncRNAs without any scores\n",
    "    lnc_with_score = lnc_with_score.drop(columns=['chr', 'start', 'end', 'strand'])\n",
    "    lnc_with_score = lnc_with_score[~lnc_with_score['lncRNA_ID'].isin(missing_scores_ids)]\n",
    "\n",
    "    # Save outputs\n",
    "    lnc_with_score.to_csv(\"conservation_feature.csv\", index=False)\n",
    "    pd.Series(list(missing_scores_ids)).to_csv(\"no_conservation_scores.csv\", index=False, header=[\"lncRNA_ID\"])\n",
    "\n",
    "    print(\"Conservation scores calculated and saved.\")\n",
    "    print(f\"conservation_feature.csv contains {len(lnc_with_score)} records.\")\n",
    "    print(f\"no_conservation_scores.csv contains {len(missing_scores_ids)} missing lncRNA IDs.\")\n",
    "\n",
    "# Usage Example\n",
    "conservation_dir = '../../omics/conservation/mouse'  # Directory containing multiple bigWig files\n",
    "calculate_conservation_features('lnc_with_position_1-based.csv', conservation_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4 Get sequence feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.1 Modify chromosome names to GENCODE standard names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chrname_mapping = pd.read_csv(\"chromosome_name_mapping.csv\")\n",
    "lncRNA = pd.read_csv(\"lnc_with_position_1-based.csv\")\n",
    "chrname_dict = dict(zip(chrname_mapping['original_name'], chrname_mapping['standard_name']))\n",
    "lncRNA['chr'] = lncRNA['chr'].replace(chrname_dict)\n",
    "\n",
    "lncRNA.to_csv(\"lnc_with_standard_chrname.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.2 Get sequence for lncRNAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyfaidx import Fasta\n",
    "import pandas as pd\n",
    "\n",
    "# file path\n",
    "genome_fasta = \"../../reference_lncRNA/reference_genome/GRCm38.p6.genome.fa\"\n",
    "lncRNA_file = \"lnc_with_standard_chrname.csv\"\n",
    "output_fasta = \"lncRNA_sequences.fasta\"\n",
    "\n",
    "# load reference genome\n",
    "genome = Fasta(genome_fasta)\n",
    "\n",
    "# load lncRNAs \n",
    "lncRNA_coords = pd.read_csv(lncRNA_file)\n",
    "\n",
    "# open output file\n",
    "with open(output_fasta, \"w\") as fasta_out:\n",
    "    for index, row in lncRNA_coords.iterrows():\n",
    "        chrom = row[\"chr\"]\n",
    "\n",
    "        start = int(row[\"start\"])\n",
    "        end = int(row[\"end\"])\n",
    "        strand = row[\"strand\"]\n",
    "        lncRNA_id = row[\"lncRNA_ID\"]\n",
    "\n",
    "        # extract sequence\n",
    "        try:\n",
    "            seq = genome[chrom][start:end].seq\n",
    "            if strand == \"-\":\n",
    "                # extract complementary sequence if strand == \"-\"\n",
    "                seq = genome[chrom][start:end].reverse.complement.seq\n",
    "\n",
    "            fasta_out.write(f\">{lncRNA_id}\\n\")\n",
    "            fasta_out.write(seq + \"\\n\")\n",
    "\n",
    "        except KeyError:\n",
    "            print(f\"Chromosome {chrom} not found in the genome file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4.2 Calculate sequence feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to seq_feature.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "def fasta_parser(fasta_file):\n",
    "    \"\"\"\n",
    "    Custom FASTA file parser to read sequences manually.\n",
    "    \"\"\"\n",
    "    with open(fasta_file, \"r\") as file:\n",
    "        identifier = None\n",
    "        sequence_lines = []\n",
    "        for line in file:\n",
    "            if line.startswith(\">\"):\n",
    "                if identifier is not None:  # Save the previous sequence before reading a new one\n",
    "                    yield SeqRecord(Seq(''.join(sequence_lines).replace(\"\\n\", \"\")), id=identifier.strip(), description='')\n",
    "                identifier = line[1:].strip()  # Extract new sequence ID, removing '>'\n",
    "                sequence_lines = []  # Reset sequence content\n",
    "            else:\n",
    "                sequence_lines.append(line.strip())\n",
    "        if identifier is not None:  # Save the last sequence\n",
    "            yield SeqRecord(Seq(''.join(sequence_lines).replace(\"\\n\", \"\")), id=identifier.strip(), description='')\n",
    "\n",
    "# Specify the input FASTA file\n",
    "input_fasta = \"lncRNA_sequences.fasta\"  # Replace with the actual FASTA file path\n",
    "\n",
    "def calculate_cpg_and_gc(sequence, window_size=None):\n",
    "    \"\"\"\n",
    "    Calculate CpG count, CpG islands, GC content, and sequence length for a given sequence.\n",
    "    \"\"\"\n",
    "    sequence = sequence.upper()  # Ensure sequence is in uppercase\n",
    "    seq_len = len(sequence)\n",
    "    if window_size is None:\n",
    "        cpg_count = sequence.count('CG')\n",
    "        cpg_islands = len(re.findall(r'CG(CG)+', sequence))\n",
    "        gc_content = (sequence.count('G') + sequence.count('C')) / seq_len if seq_len > 0 else 0\n",
    "        return {\n",
    "            \"CpG_count\": cpg_count,\n",
    "            \"CpG_islands\": cpg_islands,\n",
    "            \"GC_content\": round(gc_content * 100, 2),\n",
    "            \"Length\": seq_len\n",
    "        }\n",
    "\n",
    "# Iterate through each sequence and compute features\n",
    "results = []\n",
    "for record in fasta_parser(input_fasta):\n",
    "    seq_id = record.id\n",
    "    sequence = str(record.seq)\n",
    "    features = calculate_cpg_and_gc(sequence)  # Compute features for the entire sequence\n",
    "    results.append({\n",
    "        \"lncRNA_ID\": seq_id,\n",
    "        \"CpG_count\": features[\"CpG_count\"],\n",
    "        \"CpG_islands\": features[\"CpG_islands\"],\n",
    "        \"GC_content (%)\": features[\"GC_content\"],\n",
    "        \"Length\": features[\"Length\"]\n",
    "    })\n",
    "\n",
    "# Save computed features to a CSV file\n",
    "df = pd.DataFrame(results)\n",
    "output_csv = \"seq_feature.csv\"  # Output feature table\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5 Merge all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tissues = ['heart', 'lung', 'brain']\n",
    "\n",
    "conservation_feature = pd.read_csv('conservation_feature.csv')\n",
    "seq_feature = pd.read_csv('seq_feature.csv')\n",
    "merged_feature = pd.merge(seq_feature, conservation_feature, on=\"lncRNA_ID\", how=\"inner\")\n",
    "all_epi = merged_feature[['lncRNA_ID']]\n",
    "for tissue in tissues:\n",
    "\tepi_feature = pd.read_csv(f'{tissue}_epi.csv')\n",
    "\tot_merged_feature = pd.merge(merged_feature, epi_feature, on=\"lncRNA_ID\", how=\"inner\")\n",
    "\tall_epi = pd.merge(all_epi, epi_feature, on='lncRNA_ID', how='inner')\n",
    "\tot_merged_feature.to_csv(f\"{tissue}_annotation.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step6 Filter out invalid interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "inter_file = '../../data/LPPI/mouse/LPPI_updated.csv'\n",
    "lncRNA_annotation_file = \"heart_annotation.csv\"\n",
    "output_file = \"inter_with_valid_lnc.csv\"\n",
    "\n",
    "# Load data\n",
    "inter = pd.read_csv(inter_file)\n",
    "lncRNA_with_annotation = pd.read_csv(lncRNA_annotation_file)\n",
    "\n",
    "# Convert lncRNA_ID list to a set for faster lookup\n",
    "lncRNA_set = set(lncRNA_with_annotation['lncRNA_ID'])\n",
    "\n",
    "# Apply filtering\n",
    "def filter_node_i(node):\n",
    "    if str(node).startswith(\"l\"):\n",
    "        return node in lncRNA_set  # Check only if it starts with 'l'\n",
    "    return True  # Keep all other nodes\n",
    "\n",
    "valid_inter = inter[inter['Node_i'].apply(filter_node_i)]\n",
    "\n",
    "# Save results\n",
    "valid_inter.to_csv(output_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esslnc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
